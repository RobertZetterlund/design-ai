# -*- coding: utf-8 -*-
"""DAT410-Module3-Group50.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xpr9Nn3uc4Bsrd0cbhiH4zXWLbYaQxin

# Module 3 DAT410 Erika Nyström & Robert Zetterlund
"""

from google.colab import drive
drive.mount('/content/drive/')

PATH_TO_SHENYANG = "/content/drive/My Drive/Design of AI systems/Module3/Shenyang_labeled.csv"
PATH_TO_SHANGHAI ="/content/drive/My Drive/Design of AI systems/Module3/Shanghai_labeled.csv"
PATH_TO_BEIJING = "/content/drive/My Drive/Design of AI systems/Module3/Beijing_labeled.csv"
PATH_TO_CHENGDU = "/content/drive/My Drive/Design of AI systems/Module3/Chengdu_labeled.csv"
PATH_TO_GUANGZHOU = "/content/drive/My Drive/Design of AI systems/Module3/Guangzhou_labeled.csv"

import numpy as np
import pandas as pd

class DistanceMixin():
  def __init__(self, distance_metric="euclidean"):
    self.distance_metric = distance_metric

  #p and q are arrays of values
  def distance(self,p,q):
    if(len(p)!=len(q)):
      raise Exception("Vectors not of the same size")

    if self.distance_metric == "euclidean":
      return np.linalg.norm(p-q)
    elif self.distance_metric == "manhattan":
      ## to be implented...
      raise Exception("distance_metric 'manhattan' not yet implemented")
    else:
      raise Exception("distance_metric need to be either 'euclidean' or 'manhattan'")

class WeightMixin():
  def __init__(self,weights):
    self.weights = weights
  
  def get_weights(self,distances):
    if self.weights == "uniform":
      return np.ones_like(distances)
    elif self.weights == "distance":
      return 1 / distances
    else:
      raise Exception("weights need to be either 'uniform' or 'distance'")

class EnsureNDArrayMixin():
  def ensure_ndarray(self, collection):
    if isinstance(collection, pd.DataFrame) or isinstance(collection, pd.Series):
      return collection.to_numpy()
    elif isinstance(collection,np.ndarray):
      return collection
    else:
      raise Exception("Collection must be of type np.ndarray or pd.DataFrame, current type is:", type(collection))

class KNNClassifier(DistanceMixin, WeightMixin, EnsureNDArrayMixin):
  def __init__(self,n_neighbors=5,weights="distance",distance_metric="euclidean"):
    self.n_neighbors=n_neighbors
    self.weights=weights
    self.distance_metric = distance_metric
    super()

  def fit(self,x,y):
    """Fits the model by storing the data.

    Parameters
    ----------
    x : ndarray | DataFrame
        List of datapoints. (n_datapoints, n_features)
    y : ndarray | DataFrame
        List of correct labels. (n_datapoints, 1)
    """

    self.X = x
    self.Y = self.ensure_ndarray(y)
  
  def predict(self,x):
    """Predicts the y values for x 

    Parameters
    ----------
    x : ndarray | DataFrame
        Collection of datapoints. (n_datapoints, n_features)
    Returns
    -------
    predictions : array of the same length as ``x`` of labels 
    """
    ## init empty list of predictions, y.
    predictions = np.zeros(len(x))

    for idx, single_x in enumerate(x):

      ## calculate distances to all other points   
      distances = np.array([self.distance(single_x,p) for p in self.X])
 
      ## Instead of sorting distances and taking from that list, we partition the array 
      ## so that the first k elements are the smallest k in said array
      ## len(closest_indexes) would be self.n_neighbors long

      closest_indexes = np.argpartition(distances, self.n_neighbors)[:self.n_neighbors]

      ## we could sort the k smallest values and get their indexes, but not neccessary
      #sorted_closest_indexes = closest_indexes[np.argsort(distances[closest_indexes])]

      ## get labels for each index
      labels = self.Y[closest_indexes] 
      ## get distances of each index
      distances = distances[closest_indexes]

      # get weights based on distance list
      weights = self.get_weights(distances)

      ## create dictionary, key:: label, value:: total weight
      total_weights = {label: 0 for label in labels}

      ## for each label and distance, add weights to classes
      for label,weight in zip(labels,weights):
        total_weights[label]+= weight

      # finally, select the label with the highest value
      predictions[idx] = max(total_weights, key=lambda key: total_weights[key])

    return predictions
  
  def score(self,x,y):
    """Calculates the accuracy of the model by predicting the label of a list of datapoints x and comparing it to actual labels y.

    Parameters
    ----------
    x : ndarray | DataFrame
        Matrix of datapoints, (n_datapoints,n_features)
    y : ndarray | DataFrame
        List of correct labels

    Returns
    -------
    predictions : ndarray
      ndarray of the same length as ``x`` of labels 
    """

    x = self.ensure_ndarray(x)
    y = self.ensure_ndarray(y)

    if(len(x)!=len(y)):
      raise Exception("Vectors not of the same size")

    y_pred = self.predict(x)

    return sum([i==j for i,j in zip(y_pred,y)])/len(y)

from sklearn.base import BaseEstimator, TransformerMixin

## A transformer class that standardizes X. i.e., mean 0 and standard deviation 1
class Standardization(BaseEstimator,TransformerMixin,EnsureNDArrayMixin):
  def _standardize(self,X):
    mean = np.mean(X)
    st_dev = np.var(X)**(1/2)
    if st_dev == 0:
      return X
    else:
      return (X - mean) / st_dev

  def fit(self,X,Y):
    return self

  ## Assumes X is either a pd.DataFrame or np.ndarray
  def transform(self,X,Y=None):    
    return np.apply_along_axis(self._standardize,0,self.ensure_ndarray(X))

"""# Actual Assigment"""

from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
import numpy as np

beijing_df = pd.read_csv(PATH_TO_BEIJING)
shenyang_df = pd.read_csv(PATH_TO_SHENYANG)

## merge training set
df = beijing_df.append(shenyang_df).reset_index()
X_df = df.drop(["PM_HIGH","index"],axis=1)
Y_df = df["PM_HIGH"]

X_train, X_test, y_train, y_test = train_test_split(X_df, Y_df, test_size=0.20, random_state=42)

pipe = make_pipeline(Standardization(), KNNClassifier(n_neighbors=750, weights="distance"))
pipe.fit(X_train,y_train)

print(pipe.score(X_test,y_test))

## compare to regular KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier

pipe = make_pipeline(Standardization(), KNeighborsClassifier(750, weights="distance"))
pipe.fit(X_train,y_train)

print(pipe.score(X_test, y_test))

## Guangzhou
g_df = pd.read_csv(PATH_TO_GUANGZHOU)
g_df_X = g_df.drop("PM_HIGH",axis=1)
g_df_Y = g_df["PM_HIGH"]

print(pipe.score(g_df_X,g_df_Y))

## Shanghai
s_df = pd.read_csv(PATH_TO_SHANGHAI)
s_df_X = s_df.drop("PM_HIGH",axis=1)
s_df_Y = s_df["PM_HIGH"]

print(pipe.score(s_df_X,s_df_Y))